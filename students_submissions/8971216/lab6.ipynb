{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6 ##\n",
    "\n",
    "Kar Lok Ng\n",
    "8971216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data\n",
    "\n",
    "# this way, all instances of 'virginica' is labelled as True\n",
    "# and all instances of 'non-virginica' is labelled as False\n",
    "y = iris.target_names[iris.target] == 'virginica'\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# training the model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# attaining train/test label predictions\n",
    "y_train_pred = log_reg.predict(X_train)\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "\n",
    "# training a DummyClassifier for context\n",
    "dummy_clf = DummyClassifier()\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "y_dummy_train_pred = dummy_clf.predict(X_train)\n",
    "y_dummy_test_pred = dummy_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Confusion Matrix: \n",
      "[[72  2]\n",
      " [ 1 37]]\n",
      "\n",
      "Test Confusion Matrix: \n",
      "[[26  0]\n",
      " [ 0 12]]\n",
      "\n",
      "Train Dummy Confusion Matrix: \n",
      "[[74  0]\n",
      " [38  0]]\n",
      "\n",
      "Test Dummy Confusion Matrix: \n",
      "[[26  0]\n",
      " [12  0]]\n"
     ]
    }
   ],
   "source": [
    "cm1 = confusion_matrix(y_train, y_train_pred)\n",
    "cm2 = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "cm3 = confusion_matrix(y_train, y_dummy_train_pred)\n",
    "cm4 = confusion_matrix(y_test, y_dummy_test_pred)\n",
    "\n",
    "print(\"Train Confusion Matrix: \")\n",
    "print(cm1)\n",
    "print(\"\")\n",
    "print(\"Test Confusion Matrix: \")\n",
    "print(cm2)\n",
    "print(\"\")\n",
    "print(\"Train Dummy Confusion Matrix: \")\n",
    "print(cm3)\n",
    "print(\"\")\n",
    "print(\"Test Dummy Confusion Matrix: \")\n",
    "print(cm4)\n",
    "# Just a note for myself to keep myself sane.\n",
    "# True Negative class False    | False Negative of class True\n",
    "# False Positive of class True | True Positive of class False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the model performs quite well as compared to the dummy classifier.\n",
    "\n",
    "The dummy classifier completely ignores the input features (X_train or X_test), and simply predicts the most common feature in the given y_train/y_test data. This provides a good baseline performance to compare our logistic regression classifier to. \n",
    "\n",
    "We see that, in the test cases for the logistic regression classifier, it actually has no false negatives and false positives. In other words, it actually managed to predict the label for the flower (virginica or not virginica) accurately 100% of the time in this set. In the training data, it also performed admirably, accurately predicting the labels for all but 3 of the observations. \n",
    "\n",
    "We can take a look at these 3 observations specifically, and see if there's anything in common regarding them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setosa ------------------\n",
      "sepal length (cm)    5.006\n",
      "sepal width (cm)     3.428\n",
      "petal length (cm)    1.462\n",
      "petal width (cm)     0.246\n",
      "dtype: float64\n",
      "sepal length (cm)    0.352490\n",
      "sepal width (cm)     0.379064\n",
      "petal length (cm)    0.173664\n",
      "petal width (cm)     0.105386\n",
      "dtype: float64\n",
      "\n",
      "Versicolor ------------------\n",
      "sepal length (cm)    5.936\n",
      "sepal width (cm)     2.770\n",
      "petal length (cm)    4.260\n",
      "petal width (cm)     1.326\n",
      "dtype: float64\n",
      "sepal length (cm)    0.516171\n",
      "sepal width (cm)     0.313798\n",
      "petal length (cm)    0.469911\n",
      "petal width (cm)     0.197753\n",
      "dtype: float64\n",
      "\n",
      "Virginica ------------------\n",
      "sepal length (cm)    6.588\n",
      "sepal width (cm)     2.974\n",
      "petal length (cm)    5.552\n",
      "petal width (cm)     2.026\n",
      "dtype: float64\n",
      "sepal length (cm)    0.635880\n",
      "sepal width (cm)     0.322497\n",
      "petal length (cm)    0.551895\n",
      "petal width (cm)     0.274650\n",
      "dtype: float64\n",
      "\n",
      "The features of the 3 mislabeled data points:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>6.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>6.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)   \n",
       "46                 5.1               3.8                1.6               0.2  \\\n",
       "54                 6.5               2.8                4.6               1.5   \n",
       "108                6.7               2.5                5.8               1.8   \n",
       "\n",
       "         target  \n",
       "46       setosa  \n",
       "54   versicolor  \n",
       "108   virginica  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# false + false = 0\n",
    "# true + true = 2\n",
    "# false true/true false = 1\n",
    "# thus if there's a difference in the two lists, it would equal 1\n",
    "# unfortunately this method strips the information of what type\n",
    "# of error we have (i.e. I don't know if it's a false positive or\n",
    "# false negative)\n",
    "diff_lst = list(y_train_pred.astype('int') + y_train.astype('int'))\n",
    "\n",
    "# get their indices\n",
    "diff_idx = [i for i in range(len(diff_lst)) if diff_lst[i] == 1]\n",
    "\n",
    "# getting per-class aggregate values\n",
    "iris_data = iris.data\n",
    "iris_target = iris.target_names[iris.target]\n",
    "iris_data['target'] = iris_target\n",
    "\n",
    "# get the mean/std per class\n",
    "\n",
    "print(\"Setosa ------------------\")\n",
    "print(iris_data[iris_data['target'] == 'setosa'].iloc[:, 0:4].mean())\n",
    "print(iris_data[iris_data['target'] == 'setosa'].iloc[:, 0:4].std())\n",
    "print(\"\")\n",
    "print(\"Versicolor ------------------\")\n",
    "print(iris_data[iris_data['target'] == 'versicolor'].iloc[:, 0:4].mean())\n",
    "print(iris_data[iris_data['target'] == 'versicolor'].iloc[:, 0:4].std())\n",
    "print(\"\")\n",
    "print(\"Virginica ------------------\")\n",
    "print(iris_data[iris_data['target'] == 'virginica'].iloc[:, 0:4].mean())\n",
    "print(iris_data[iris_data['target'] == 'virginica'].iloc[:, 0:4].std())\n",
    "print(\"\")\n",
    "print('The features of the 3 mislabeled data points:')\n",
    "display((iris.data.iloc[diff_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My idea going into this may be that each of the erronious labelling may be due to the fact that the features may be outliers in their respective categories. However, it does not seem like that is the case here - all features of our 3 wrongly labelled observations lie at most around 1 standard deviation away from the mean. \n",
    "\n",
    "Perhaps these 3 observations may be close to the decision boundary between viriginica and not-virginica, and thus may be labelled wrongly by our model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCN8010_classic_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
